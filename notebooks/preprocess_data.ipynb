{"cells":[{"cell_type":"markdown","metadata":{"id":"VFuMzfXwCLnZ"},"source":["### **Step 0:** Import necessary libraries and connect to Google Drive for file access and output storage."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10368,"status":"ok","timestamp":1736276314275,"user":{"displayName":"יהונתן קידושים","userId":"10685561410932823635"},"user_tz":-120},"id":"027FCZv7CLna","outputId":"b09abfa9-dd1d-4bc2-d02b-9ea1133337d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Adjusting current working directory to parent directory\n","from pathlib import Path\n","from os import chdir\n","from platform import system\n","\n","try:\n","    current_directory\n","except: # First  run - initialize current_directory\n","    current_directory = Path.cwd()\n","    if system() == \"Linux\": # Colab\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        current_directory = f\"{current_directory}/drive/MyDrive/Colab Notebooks/RecTour2024Challenge\"\n","    else:\n","        current_directory = current_directory.parent\n","finally:\n","    chdir(current_directory)\n","\n","\n","\n","# External imports\n","import pandas as pd\n","import numpy as np\n","\n","from random import randint\n","\n","import torch\n","import torch.nn as nn\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from collections import defaultdict\n","\n","\n","# Internal imports\n","from src.data.csv_tools import csv_to_dataframe, dataframe_to_csv, save_submission\n","from src.data.pickle_tools import save_to_pickle, load_pickle\n","from src.data.keras_tools import save_keras_model_weights, load_keras_model_weights\n","from src.utils.preprocessing_tools import *"]},{"cell_type":"markdown","metadata":{"id":"mgu4_fVFCLnb"},"source":["### **Step 1:** Aggregate reviews for each accommodation: compile all relevant reviews into a single dataset for each accommodation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FshX5SvJCLnb"},"outputs":[],"source":["# aggregate reviews for each accommodation as a list\n","for set_name in [\"train\", \"val\", \"test\"]:\n","    try:\n","        accommodation_reviews = load_pickle(f\"{set_name}_reviews_grouped_by_accommodation\")\n","    except:\n","        accommodation_reviews = create_accommodation_reviews(set_name)"]},{"cell_type":"markdown","metadata":{"id":"hMFptJNYcOy-"},"source":["### **Step 2:** Match users by userid and merge with reviews using reviewid, then create dictionaries where the key is the userid and the value is the reviewid."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouOG1Qa9CLnb"},"outputs":[],"source":["# Concatenate train and val sets\n","train_processed_set, val_processed_set = None, None\n","try:\n","    train_processed_set = csv_to_dataframe(\"train\")\n","except:\n","    train_processed_set = create_concatenated_set(\"train\")\n","try:\n","    val_processed_set = csv_to_dataframe(\"val\")\n","except:\n","    val_processed_set = create_concatenated_set(\"val\")\n","\n","train_user_review_dict = {}\n","try:\n","    train_user_review_dict = load_pickle(\"train_user_review_dict\")\n","except:\n","    for index, row in train_processed_set.iterrows():\n","        user_id = row[\"user_id\"]\n","        review_id = row[\"review_id\"]\n","        train_user_review_dict[user_id] = review_id\n","    save_to_pickle(train_user_review_dict, \"train_user_review_dict\")\n","\n","val_user_review_dict = {}\n","try:\n","    val_user_review_dict = load_pickle(\"val_user_review_dict\")\n","except:\n","    for index, row in val_processed_set.iterrows():\n","        user_id = row[\"user_id\"]\n","        review_id = row[\"review_id\"]\n","        val_user_review_dict[user_id] = review_id\n","    save_to_pickle(val_user_review_dict, \"val_user_review_dict\")"]},{"cell_type":"markdown","metadata":{"id":"Qxo4m9TxCLnc"},"source":["### **STEP 3:** Handling missing values in review data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2875,"status":"ok","timestamp":1736276333653,"user":{"displayName":"יהונתן קידושים","userId":"10685561410932823635"},"user_tz":-120},"id":"dE-5KkCVCLnc","outputId":"137e1f77-83e8-4c59-dc88-f3e8ec32a278"},"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values in train:\n","review_title       0\n","review_positive    0\n","review_negative    0\n","dtype: int64\n","Missing values in val:\n","review_title       0\n","review_positive    0\n","review_negative    0\n","dtype: int64\n","Missing values in test:\n","review_title       0\n","review_positive    0\n","review_negative    0\n","dtype: int64\n"]}],"source":["# Fill na with empty strings\n","train_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]] = train_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].fillna(\"\")\n","val_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]] = val_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].fillna(\"\")\n","# For test, get reviews data\n","test_reviews = csv_to_dataframe(\"test\", \"reviews\")\n","test_reviews[[\"review_title\", \"review_positive\", \"review_negative\"]] = test_reviews[[\"review_title\", \"review_positive\", \"review_negative\"]].fillna(\"\")\n","# Validate that there are no missing values\n","print(\"Missing values in train:\")\n","print(train_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].isna().sum())\n","print(\"Missing values in val:\")\n","print(val_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].isna().sum())\n","print(\"Missing values in test:\")\n","print(test_reviews[[\"review_title\", \"review_positive\", \"review_negative\"]].isna().sum())"]},{"cell_type":"markdown","source":["### **Throughout our work, we explored a total of 18 models. Some featured minor adjustments, while others took us in entirely different directions. In this notebook, we will showcase three main approaches that guided our experimentation.**"],"metadata":{"id":"WoToX6swCkn-"}},{"cell_type":"markdown","source":["\n","# ***First + Seconde approach - Ensemble of three models: Titles, Positive Reviews, and Negative Reviews***\n","\n","In this section, you will see shared code snippets applicable to both approaches until we reach the point where the two approaches diverge.\n"],"metadata":{"id":"C5jLt3NICyuZ"}},{"cell_type":"markdown","metadata":{"id":"fMo6OIq-6rru"},"source":["### Sanity Check: Encoding Review Data with SentenceTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Jk5vrwLCLnc"},"outputs":[],"source":["model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Sanity check\n","sanity_dataset = val_processed_set.head()\n","title_vectors = model.encode(sanity_dataset[\"review_title\"].values)\n","positive_vectors = model.encode(sanity_dataset[\"review_positive\"].values)\n","negative_vectors = model.encode(sanity_dataset[\"review_negative\"].values)\n","print(title_vectors.shape)\n","print(positive_vectors.shape)\n","print(negative_vectors.shape)"]},{"cell_type":"markdown","metadata":{"id":"veWnpF1JcVbR"},"source":["### Encoding and Saving Review Data Vectors for Train, Validation, and Test Sets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23510,"status":"ok","timestamp":1736276357161,"user":{"displayName":"יהונתן קידושים","userId":"10685561410932823635"},"user_tz":-120},"id":"EeiBcOhvCLnd","outputId":"0312fea9-b29e-4e36-aa5c-bca38ec2accf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting train\n","Finished title.\n","Finished positive.\n","Finished negative.\n","Starting val\n","Finished title.\n","Finished positive.\n","Finished negative.\n","Starting test\n","Finished title.\n","Finished positive.\n","Finished negative.\n"]}],"source":["# Get pairs for each set\n","set_name_data_pair = [(\"train\", train_processed_set), (\"val\", val_processed_set), (\"test\", test_reviews)]\n","\n","for set_name, set_data in set_name_data_pair:\n","    print(\"Starting\", set_name)\n","\n","    review_ids = set_data[\"review_id\"].values\n","    try:\n","      load_pickle(f\"{set_name}_title_vectors_dict\")\n","    except:\n","      title_vectors = model.encode(set_data[\"review_title\"].values, show_progress_bar=True)\n","      title_vectors_dict = dict(zip(review_ids, title_vectors))\n","      save_to_pickle(title_vectors_dict, f\"{set_name}_title_vectors_dict\")\n","    print(f\"Finished title.\")\n","\n","    try:\n","      load_pickle(f\"{set_name}_positive_vectors_dict\")\n","    except:\n","      positive_vectors = model.encode(set_data[\"review_positive\"].values, show_progress_bar=True)\n","      positive_review_vectors_dict = dict(zip(review_ids, positive_vectors))\n","      save_to_pickle(positive_review_vectors_dict, f\"{set_name}_positive_vectors_dict\")\n","    print(f\"Finished positive.\")\n","\n","    try:\n","      load_pickle(f\"{set_name}_negative_vectors_dict\")\n","    except:\n","      negative_vectors = model.encode(set_data[\"review_negative\"].values, show_progress_bar=True)\n","      negative_review_vectors_dict = dict(zip(review_ids, negative_vectors))\n","      save_to_pickle(negative_review_vectors_dict, f\"{set_name}_negative_vectors_dict\")\n","    print(f\"Finished negative.\")"]},{"cell_type":"markdown","metadata":{"id":"g1qJMRj9CLnd"},"source":["### Create country embeddings for users"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5G9g5ULKCLnd"},"outputs":[],"source":["# Get all unique countries from train and val\n","users_countries_embedding_dict = {}\n","try:\n","    users_countries_embedding_dict = load_pickle(\"users_countries_embedding_dict\")\n","except:\n","    train_countries = train_processed_set[\"guest_country\"].unique()\n","    val_countries = val_processed_set[\"guest_country\"].unique()\n","    all_countries = np.unique(np.concatenate((train_countries, val_countries)))\n","    embedding = nn.Embedding(len(all_countries), 8)                                   # We chose an embedding size of 8 because the unique values for guest_country can be effectively represented using a vector of size 8.\n","    for country_index, country in enumerate(all_countries):\n","        country_embedding = embedding(torch.tensor(country_index)).detach().numpy()\n","        users_countries_embedding_dict[country] = country_embedding\n","    save_to_pickle(users_countries_embedding_dict, \"users_countries_embedding_dict\")"]},{"cell_type":"markdown","source":["### Create country embeddings for accommodations"],"metadata":{"id":"Xs6-90-1EPG0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pc6ymr2z71aK"},"outputs":[],"source":["accommodation_countries_embedding_dict = {}\n","try:\n","  accommodation_countries_embedding_dict = load_pickle(\"accommodation_countries_embedding_dict\")\n","except:\n","  train_countries = train_processed_set[\"accommodation_country\"].unique()\n","  val_countries = val_processed_set[\"accommodation_country\"].unique()\n","  test_users = csv_to_dataframe(\"test\", \"users\")\n","  test_countries = test_users[\"accommodation_country\"].unique()\n","  all_countries = np.unique(np.concatenate((train_countries, val_countries, test_countries)))\n","  embedding = nn.Embedding(len(all_countries), 8)                                                               # We chose an embedding size of 8 because the unique values for accommodation_country can be effectively represented using a vector of size 8.\n","  for country_index, country in enumerate(all_countries):\n","    country_embedding = embedding(torch.tensor(country_index)).detach().numpy()\n","    accommodation_countries_embedding_dict[country] = country_embedding\n","  save_to_pickle(accommodation_countries_embedding_dict, \"accommodation_countries_embedding_dict\")"]},{"cell_type":"markdown","metadata":{"id":"zRIodJXFCLnd"},"source":["### Create guest type embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NGdtqSqCLne"},"outputs":[],"source":["# Get unique guest_types\n","guest_types_embedding_dict = {}\n","try:\n","    guest_types_embedding_dict = load_pickle(\"guest_types_embedding_dict\")\n","except:\n","    train_guest_types = train_processed_set[\"guest_type\"].unique()\n","    val_guest_types = val_processed_set[\"guest_type\"].unique()\n","    all_guest_types = np.unique(np.concatenate((train_guest_types, val_guest_types)))\n","    embedding = nn.Embedding(len(all_guest_types), 2)                                                     # We chose an embedding size of 2 because the unique values for guest_type can be effectively represented using a vector of size 2.\n","    for guest_type_index, guest_type in enumerate(all_guest_types):\n","        guest_type_embedding = embedding(torch.tensor(guest_type_index)).detach().numpy()\n","        guest_types_embedding_dict[guest_type] = guest_type_embedding\n","    save_to_pickle(guest_types_embedding_dict, \"guest_types_embedding_dict\")"]},{"cell_type":"markdown","source":["### Create accommodation type embeddings"],"metadata":{"id":"d94xSEwKEjbw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVbmmU4z-spW"},"outputs":[],"source":["accommodation_type_embedding_dict = {}\n","try:\n","  accommodation_type_embedding_dict = load_pickle(\"accommodation_type_embedding_dict\")\n","except:\n","  train_acc_types = train_processed_set[\"accommodation_type\"].unique()\n","  val_acc_types = val_processed_set[\"accommodation_type\"].unique()\n","  test_users = csv_to_dataframe(\"test\", \"users\")\n","  test_acc_types = test_users[\"accommodation_type\"].unique()\n","  all_acc_types = np.unique(np.concatenate((train_acc_types, val_acc_types, test_acc_types)))\n","  embedding = nn.Embedding(len(all_acc_types), 5)                                                     # We chose an embedding size of 5 because the unique values for accommodation_type can be effectively represented using a vector of size 5.\n","  for acc_types_index, acc_types in enumerate(all_acc_types):\n","    acc_types_embedding = embedding(torch.tensor(acc_types_index)).detach().numpy()\n","    accommodation_type_embedding_dict[acc_types] = acc_types_embedding\n","  save_to_pickle(accommodation_type_embedding_dict, \"accommodation_type_embedding_dict\")"]},{"cell_type":"markdown","metadata":{"id":"W7ZKOpXnCLne"},"source":["### Create user embedding for train and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCWY2wm1CLne"},"outputs":[],"source":["def get_processed_test_users():\n","    test_users = csv_to_dataframe(\"test\", \"users\")\n","    test_users['guest_country'] = test_users['guest_country'].fillna(\"EMPTY\")\n","    return test_users\n","\n","set_name_data_pair[2] = (\"test\", get_processed_test_users())\n","for set_name, set_df in set_name_data_pair:\n","  try:\n","      current_users_embeddings_dict = load_pickle(f\"{set_name}_user_embeddings_dict\")\n","  except:\n","    current_users_embeddings_dict = {}\n","    for _, row in set_df.iterrows():\n","        user_id = row[\"user_id\"]\n","        embedded_guest_country = users_countries_embedding_dict[row[\"guest_country\"]]\n","        embedded_guest_type = guest_types_embedding_dict[row[\"guest_type\"]]\n","        embedded_month = row[\"month\"] / 12                                                                        # We performed normalization by dividing by the maximum value.\n","        embedded_room_nights = row[\"room_nights\"] / 112                                                           # We performed normalization by dividing by the maximum value.\n","        embedded_acc_country = accommodation_countries_embedding_dict[row[\"accommodation_country\"]]\n","        embedded_acc_type = accommodation_type_embedding_dict[row[\"accommodation_type\"]]\n","        current_users_embeddings_dict[user_id] = np.concatenate((embedded_guest_country, embedded_guest_type, np.array([embedded_month, embedded_room_nights]), embedded_acc_country, embedded_acc_type))\n","    save_to_pickle(current_users_embeddings_dict, f\"{set_name}_user_embeddings_dict\")"]},{"cell_type":"markdown","source":["### Sanity check to validate embedding size"],"metadata":{"id":"tCwSSIPHEzh8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1736199161589,"user":{"displayName":"יהונתן קידושים","userId":"10685561410932823635"},"user_tz":-120},"id":"coB92mF9Y5Kl","outputId":"a3b376df-57a9-4841-944f-80c267a2f76b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25,)"]},"metadata":{},"execution_count":11}],"source":["list(current_users_embeddings_dict.values())[0].shape"]},{"cell_type":"markdown","metadata":{"id":"eyPSS_2gMqAq"},"source":["### Create np array that each cell contains user embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4965,"status":"ok","timestamp":1736072774584,"user":{"displayName":"יהונתן קידושים","userId":"10685561410932823635"},"user_tz":-120},"id":"GXEFjFlNb5AF","outputId":"bd885ebf-e3db-41cb-c8dd-5e61eb335ef0"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1628989, 25)\n","(203787, 25)\n"]}],"source":["train_user_embeddings_array = np.array(list(load_pickle(\"train_user_embeddings_dict\").values()))\n","print(train_user_embeddings_array.shape)\n","val_user_embeddings_array = np.array(list(load_pickle(\"val_user_embeddings_dict\").values()))\n","print(val_user_embeddings_array.shape)"]},{"cell_type":"markdown","source":["### Load all section reviews embeddigns"],"metadata":{"id":"DSLh7K0KFzv2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9x_R1xsVCLnf"},"outputs":[],"source":["# Load all section reviews embeddigns\n","train_title_vectors_dict = load_pickle(\"train_title_vectors_dict\")\n","train_positive_vectors_dict = load_pickle(\"train_positive_vectors_dict\")\n","train_negative_vectors_dict = load_pickle(\"train_negative_vectors_dict\")\n","val_title_vectors_dict = load_pickle(\"val_title_vectors_dict\")\n","val_positive_vectors_dict = load_pickle(\"val_positive_vectors_dict\")\n","val_negative_vectors_dict = load_pickle(\"val_negative_vectors_dict\")"]},{"cell_type":"markdown","source":["### Create embedding arrays for train and validation datasets"],"metadata":{"id":"JkSTjSP9GDG1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MO2d70LSpFkH"},"outputs":[],"source":["reviews_embeddings_arrays = {\n","    \"train\": {\n","        \"users\": train_user_embeddings_array,\n","        \"title\": np.array(list(train_title_vectors_dict.values())),\n","        \"positive\": np.array(list(train_positive_vectors_dict.values())),\n","        \"negative\": np.array(list(train_negative_vectors_dict.values())),\n","        \"labels\": np.concatenate([np.ones(len(train_positive_vectors_dict)), np.zeros(len(train_positive_vectors_dict) * 3)], axis=0)\n","    },\n","    \"val\": {\n","        \"users\": val_user_embeddings_array,\n","        \"title\": np.array(list(val_title_vectors_dict.values())),\n","        \"positive\": np.array(list(val_positive_vectors_dict.values())),\n","        \"negative\": np.array(list(val_negative_vectors_dict.values())),\n","        \"labels\": np.concatenate([np.ones(len(val_positive_vectors_dict)), np.zeros(len(val_positive_vectors_dict) * 3)], axis=0)\n","    }\n","}"]},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"id":"xX3f_Q4_Gqyi"}},{"cell_type":"code","source":["def get_correct_pairs(set_name, section_name):\n","    return reviews_embeddings_arrays[set_name][\"users\"], reviews_embeddings_arrays[set_name][section_name]\n","\n","def get_set_labels(section_name):\n","    return reviews_embeddings_arrays[section_name][\"labels\"]"],"metadata":{"id":"ftDvzX0hGqDI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**First approach** – We proposed a contrastive learning method where negative samples are selected based on a similarity threshold with the positive sample, rather than randomly. This approach aims to provide the model with more challenging negative samples for improved training effectiveness.\n","\n","In our experiments, we tested different thresholds and varied the number of negative samples between 2 and 4 to evaluate their impact on the model's performance. The presented code snippet is one example of the implementation."],"metadata":{"id":"WFc9jrSdHyfR"}},{"cell_type":"markdown","source":["### Generating hard negative review samples based on cosine similarity"],"metadata":{"id":"hwoHgaHPKC1v"}},{"cell_type":"code","source":["def cosine_similarity(vec1, vec2):\n","    dot_product = np.dot(vec1, vec2)\n","    norm_vec1 = np.linalg.norm(vec1)\n","    norm_vec2 = np.linalg.norm(vec2)\n","    x = dot_product / (norm_vec1 * norm_vec2)\n","    return x\n","\n","def create_negative_lists(coorect_pairs, reviews_embeddings_users_dict, embeddings_list, set_name, section_name):\n","    \"\"\"\n","    Parameters:\n","    - coorect_pairs: List of tuples containing user embeddings and their associated review embeddings.\n","    - reviews_embeddings_users_dict: Dictionary mapping review embeddings to user embeddings.\n","    - embeddings_list: List of all review embeddings in the dataset.\n","    - set_name: Name of the dataset (e.g., \"train\", \"val\").\n","    - section_name: Specific section of the dataset being processed.\n","\n","    Returns:\n","    - Three numpy arrays of negative review embeddings for each pair.\n","    \"\"\"\n","\n","    amount_of_reviews = len(coorect_pairs)\n","    last_review_index = amount_of_reviews - 1\n","    print_steps = amount_of_reviews // 20\n","    current_print = 0\n","    negative_reviews1, negative_reviews2, negative_reviews3 = [], [], []\n","    for current_review_index, (user_embedding, review) in enumerate(coorect_pairs, start=1):\n","\n","        if current_review_index % print_steps == 0:\n","            print(f\"{set_name}-{section_name}: {current_print}% completed\")\n","            current_print += 5\n","\n","        user_embedding = tuple(user_embedding)\n","\n","        # Generate the first negative review with similarity < 0.35\n","        negative_review1 = embeddings_list[randint(0, last_review_index)]\n","        while cosine_similarity(negative_review1, review) < 0.35:\n","            negative_review1 = embeddings_list[randint(0, last_review_index)]\n","\n","        # Generate the second negative review with similarity < 0.35\n","        negative_review2 = embeddings_list[randint(0, last_review_index)]\n","        while cosine_similarity(negative_review2, review) < 0.35:\n","            negative_review2 = embeddings_list[randint(0, last_review_index)]\n","\n","        # Generate the third negative review with similarity < 0.35\n","        negative_review3 = embeddings_list[randint(0, last_review_index)]\n","        while cosine_similarity(negative_review3, review) < 0.35:\n","            negative_review3 = embeddings_list[randint(0, last_review_index)]\n","\n","        # Append generated negative reviews to respective lists\n","        negative_reviews1.append(negative_review1)\n","        negative_reviews2.append(negative_review2)\n","        negative_reviews3.append(negative_review3)\n","\n","    # Return the generated negative review embeddings as numpy arrays\n","    return np.array(negative_reviews1), np.array(negative_reviews2), np.array(negative_reviews3)"],"metadata":{"id":"AlBBhdYtHmR9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proposed architecture in approach 1"],"metadata":{"id":"go0O_JO6J98X"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n","\n","def create_contrastive_model():\n","\n","  user_input = Input(shape=(25,), name='user')\n","  user_dense = Dense(128, activation='relu', name='user_dense_1')(user_input)\n","  user_dense = Dense(64, activation='relu', name='user_dense_2')(user_dense)\n","\n","  review_input = Input(shape=(384,), name='review')\n","  review_dense = Dense(256, activation='relu', name='review_dense_1')(review_input)\n","  review_dense = Dense(64, activation='relu', name='review_dense_2')(review_dense)\n","\n","  combined = Concatenate(name='concatenate_layer')([user_dense, review_dense])\n","\n","  combined_dense = Dense(128, activation='relu', name='combined_dense_1')(combined)\n","  combined_dense = Dropout(0.3, name='dropout_layer')(combined_dense)\n","  combined_dense = Dense(64, activation='relu', name='combined_dense_2')(combined_dense)\n","\n","  output = Dense(1, activation='sigmoid', name='output_layer')(combined_dense)\n","\n","  return Model([user_input, review_input], output, name='contrastive_model')"],"metadata":{"id":"ALX-T0KuJvLv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Second approach** - We proposed a contrastive learning method where negative samples are selected to ensure they do not belong to the same user group as the positive sample and are distinct from each other. The model uses a dot product to measure the relationship between user and review embeddings, and training is guided by a custom contrastive loss function.\n","\n","In our experiments, we varied the number of negative samples between 2 and 4 to evaluate their impact on the model's performance. The presented code snippet is one example of the implementation.\n","\n"],"metadata":{"id":"VTTpzGowMtJl"}},{"cell_type":"markdown","source":["### Generating negative review samples"],"metadata":{"id":"TEzwHJ6cNStQ"}},{"cell_type":"code","source":["def create_negative_lists(coorect_pairs, reviews_embeddings_users_dict, embeddings_list, set_name, section_name):\n","    \"\"\"\n","    Parameters:\n","    - coorect_pairs: List of tuples containing user embeddings and their associated review embeddings.\n","    - reviews_embeddings_users_dict: Dictionary mapping review embeddings to user embeddings.\n","    - embeddings_list: List of all review embeddings in the dataset.\n","    - set_name: Name of the dataset (e.g., \"train\", \"val\").\n","    - section_name: Specific section of the dataset being processed.\n","\n","    Returns:\n","    - Three numpy arrays of negative review embeddings for each pair.\n","    \"\"\"\n","\n","    amount_of_reviews = len(coorect_pairs)\n","    last_review_index = amount_of_reviews - 1\n","    print_steps = amount_of_reviews // 20\n","    current_print = 0\n","    negative_reviews1, negative_reviews2, negative_reviews3 = [], [], []\n","    for current_review_index, (user_embedding, review) in enumerate(coorect_pairs, start=1):\n","\n","        if current_review_index % print_steps == 0:\n","            print(f\"{set_name}-{section_name}: {current_print}% completed\")\n","            current_print += 5\n","\n","        user_embedding = tuple(user_embedding)\n","\n","        negative_review1 = embeddings_list[randint(0, last_review_index)]\n","        while user_embedding in reviews_embeddings_users_dict[tuple(negative_review1)]:\n","            negative_review1 = embeddings_list[randint(0, last_review_index)]\n","\n","        negative_review2 = embeddings_list[randint(0, last_review_index)]\n","        while np.all(negative_review1 == negative_review2) or user_embedding in reviews_embeddings_users_dict[tuple(negative_review2)]:\n","            negative_review2 = embeddings_list[randint(0, last_review_index)]\n","\n","        negative_review3 = embeddings_list[randint(0, last_review_index)]\n","        while np.all(negative_review1 == negative_review3) or np.all(negative_review2 == negative_review3) or user_embedding in reviews_embeddings_users_dict[tuple(negative_review3)]:\n","            negative_review3 = embeddings_list[randint(0, last_review_index)]\n","\n","        negative_reviews1.append(negative_review1)\n","        negative_reviews2.append(negative_review2)\n","        negative_reviews3.append(negative_review3)\n","\n","    return np.array(negative_reviews1), np.array(negative_reviews2), np.array(negative_reviews3)"],"metadata":{"id":"PZB8zFOOKbZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proposed architecture and loss function in approach 2"],"metadata":{"id":"Pag9Y2CWNOK7"}},{"cell_type":"code","source":["def contrastive_loss(y_true, y_pred, margin=1.0):\n","    # Loss for positive pairs: minimize squared distance\n","    square_pred = tf.square(y_pred)\n","    # Loss for negative pairs: penalize if distance < margin\n","    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n","    # Combine positive and negative loss\n","    return tf.reduce_mean((1 - y_true) * square_pred + y_true * margin_square)\n","\n","def create_contrastive_model():\n","    user_input = Input(shape=(25,), name='user')\n","    user_dense = Dense(384, activation='relu', name='user_dense')(user_input)\n","\n","    review_input = Input(shape=(384,), name='review')\n","\n","    # Compute the dot product between user and review embeddings\n","    dotProduct = Lambda(lambda tensors: tf.reduce_sum(tensors[0] * tensors[1], axis=-1, keepdims=True)) \\\n","                              ([user_dense, review_input])\n","\n","    return Model([user_input, review_input], dotProduct, name='contrastive_model')"],"metadata":{"id":"BqjiDyh4Kvav"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training the model – shared for both approaches (difference in loss function)"],"metadata":{"id":"wln7ZRfFKJwd"}},{"cell_type":"code","source":["def create_section_model(section_name, batch_size=512, epochs=15):\n","    print(\"Creating training data\")\n","    train_users, train_section_reviews = get_correct_pairs(\"train\", section_name)\n","    reviews_embeddings_users_dict_train = defaultdict(set)\n","    coorect_pairs = list(zip(train_users, train_section_reviews))\n","    for user, review in coorect_pairs:\n","      review_key = tuple(review)\n","      reviews_embeddings_users_dict_train[review_key].add(tuple(user))\n","    train_negative1, train_negative2, train_negative3 = create_negative_lists(coorect_pairs, reviews_embeddings_users_dict_train, train_section_reviews, \"train\", section_name)\n","    train_all_users = np.concatenate([train_users, train_users, train_users, train_users], axis=0)\n","    train_all_section_reviews = np.concatenate([train_section_reviews, train_negative1, train_negative2, train_negative3], axis=0)\n","    train_all_labels = get_set_labels(\"train\")\n","\n","    print(\"Creating validation data\")\n","    val_users, val_section_reviews = get_correct_pairs(\"val\", section_name)\n","    reviews_embeddings_users_dict_val = defaultdict(set)\n","    coorect_pairs = list(zip(val_users, val_section_reviews))\n","    for user, review in coorect_pairs:\n","      review_key = tuple(review)\n","      reviews_embeddings_users_dict_val[review_key].add(tuple(user))\n","    val_negative1, val_negative2, val_negative3 = create_negative_lists(coorect_pairs, reviews_embeddings_users_dict_val, val_section_reviews, \"val\", section_name)\n","    val_all_users = np.concatenate([val_users, val_users, val_users, val_users], axis=0)\n","    val_all_section_reviews = np.concatenate([val_section_reviews, val_negative1, val_negative2, val_negative3], axis=0)\n","    val_all_labels = get_set_labels(\"val\")\n","\n","    model = create_contrastive_model()\n","\n","    model.compile(optimizer='adam', loss='binary_crossentropy') # or model.compile(optimizer='adam', loss='contrastive_loss')\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n","\n","    history = model.fit(\n","        [train_all_users, train_all_section_reviews], train_all_labels,\n","        batch_size=batch_size,\n","        epochs=epochs,\n","        validation_data=([val_all_users, val_all_section_reviews], val_all_labels),\n","        callbacks=[early_stopping, lr_scheduler]\n","    )\n","    return model"],"metadata":{"id":"XYIlo3SiJkHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQzLcdUTsBjN"},"source":["### Creating the 3 models - shared for both approaches"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":950322,"status":"ok","timestamp":1736035718424,"user":{"displayName":"יהונתן קידושים","userId":"10685561410932823635"},"user_tz":-120},"id":"cijhh-QouQgt","outputId":"b0c09cd0-112c-4a0f-e093-885db4b3b5e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating training data\n","train-positive: 0% completed\n","train-positive: 5% completed\n","train-positive: 10% completed\n","train-positive: 15% completed\n","train-positive: 20% completed\n","train-positive: 25% completed\n","train-positive: 30% completed\n","train-positive: 35% completed\n","train-positive: 40% completed\n","train-positive: 45% completed\n","train-positive: 50% completed\n","train-positive: 55% completed\n","train-positive: 60% completed\n","train-positive: 65% completed\n","train-positive: 70% completed\n","train-positive: 75% completed\n","train-positive: 80% completed\n","train-positive: 85% completed\n","train-positive: 90% completed\n","train-positive: 95% completed\n","Creating validation data\n","val-positive: 0% completed\n","val-positive: 5% completed\n","val-positive: 10% completed\n","val-positive: 15% completed\n","val-positive: 20% completed\n","val-positive: 25% completed\n","val-positive: 30% completed\n","val-positive: 35% completed\n","val-positive: 40% completed\n","val-positive: 45% completed\n","val-positive: 50% completed\n","val-positive: 55% completed\n","val-positive: 60% completed\n","val-positive: 65% completed\n","val-positive: 70% completed\n","val-positive: 75% completed\n","val-positive: 80% completed\n","val-positive: 85% completed\n","val-positive: 90% completed\n","val-positive: 95% completed\n"]}],"source":["for section_name in [\"title\", \"negative\", \"positive\"]:\n","    section_model = create_section_model(section_name)\n","    section_model.save(f\"out/models/{section_name}_3_negative_with_embedded_of_acc_with_binary_crossentropy.h5\")"]},{"cell_type":"markdown","source":["# ***Third approach***\n","\n","In this approach, we leverage a pre-trained transformer model to create embeddings for user and review descriptions. Using these embeddings, we train the model with a Multiple Negatives Ranking Loss, which optimizes the model to associate the correct user-review pairs while minimizing similarity to negative examples. The training process involves generating structured input examples from the dataset, pairing user and review embeddings, and fine-tuning the transformer with a ranking objective.\n","\n","In our experiments, we tested various combinations of features to describe the user and the review. The code snippet here demonstrates one example of these combinations.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"8ee4zvJdOWGJ"}},{"cell_type":"markdown","source":["### Importing libraries and installing dependencies"],"metadata":{"id":"fh6s3lwzPhSd"}},{"cell_type":"code","source":["!pip install sentence-transformers torch pandas numpy\n","!pip install datasets\n","from sentence_transformers import SentenceTransformer, InputExample, losses\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","import numpy as np\n","from datasets import Dataset"],"metadata":{"id":"k8rUNNfQtF7p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preparing training examples with user and review descriptions"],"metadata":{"id":"j0FFDQC7PkVR"}},{"cell_type":"code","source":["train_examples = []\n","for i, row in train_processed_set.iterrows():\n","    user_desc = f\"{row['guest_country']} {row['guest_type']} {row['month']} {row['room_nights']} {row['accommodation_country']} {row['accommodation_type']}\"\n","    review_desc = f\"{row['review_title']} {row['review_positive']} {row['review_negative']}\"\n","    train_examples.append(InputExample(texts=[review_desc , user_desc]))"],"metadata":{"id":"ctoy08SUtIiq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training the model with Multiple Negatives Ranking Loss"],"metadata":{"id":"A4AB1i2GPp1J"}},{"cell_type":"code","source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)\n","model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","train_loss = losses.MultipleNegativesRankingLoss(model)\n","model.fit(\n","    train_objectives=[(train_dataloader, train_loss)],\n","    epochs=3,\n","    optimizer_params={'lr': 2e-5, 'weight_decay': 0.01 },\n","    warmup_steps=((len(train_examples) / 64) * 3) * 0.05,\n","    show_progress_bar=True\n",")\n","\n","model.save(\"out/models/user_review_matching_model2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1e846bfc472746e08f0d6cad1b04cf6d","87b2411d0c684cd6a83e3f4ecfa04555","753d14a31bed472ebef84c5b07314c3e","5614c8d6dab54fd6aa3d70350fb639c7","6d727798ccd748eaa8a5ba87c88570b6","8d48991b6c6d40ef85ab13b583de9200","6aa877f961ed46dc9906a64d328b102d","4fca820ac3da4ebfbf4966efd3fe5147","492c57272f3246ca8e46833d82238215","ce126d1b27504318b061e2371ab731fd","0f69d181f73e458fae2113616d00ff9e"]},"id":"ytAtpxc2guDP","executionInfo":{"status":"ok","timestamp":1736286239608,"user_tz":-120,"elapsed":9738900,"user":{"displayName":"יהונתן קידושים","userId":"10685561410932823635"}},"outputId":"43ce496f-d82b-485c-f9a4-a8d450376f4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='76359' max='76359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [76359/76359 2:41:55, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.821800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.381100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.122600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.951100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>2.805400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>2.651700</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>2.526900</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>2.448300</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>2.351200</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>2.305000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>2.254900</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>2.220400</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>2.161200</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>2.136100</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>2.111000</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>2.087600</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>2.061700</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>2.035900</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>2.024900</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>2.007400</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>1.998000</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>1.982100</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>1.946400</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>1.965400</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>1.911100</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>1.931900</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>1.905000</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>1.900800</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>1.889900</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>1.882100</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>1.868000</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>1.873900</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>1.846800</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>1.843300</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>1.854300</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>1.828900</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>1.828800</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>1.835600</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>1.817500</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>1.813800</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>1.806700</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>1.799700</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>1.793800</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>1.792800</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>1.772600</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>1.767100</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>1.773200</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>1.769500</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>1.753900</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>1.772200</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>1.763600</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>1.720200</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>1.728700</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>1.714600</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>1.709300</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>1.704200</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>1.689300</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>1.712700</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>1.692400</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>1.693500</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>1.685700</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>1.693300</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>1.683500</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>1.664000</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>1.679900</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>1.674800</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>1.685600</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>1.675200</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>1.660700</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>1.662700</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>1.666100</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>1.663000</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>1.654900</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>1.650800</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>1.642400</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>1.656200</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>1.657000</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>1.649500</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>1.634100</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>1.652900</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>1.623900</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>1.634300</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>1.637900</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>1.634300</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>1.621900</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>1.621600</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>1.629200</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>1.627800</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>1.635200</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>1.628200</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>1.629900</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>1.623000</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>1.635300</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>1.606400</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>1.619000</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>1.617600</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>1.614200</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>1.599000</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>1.620700</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>1.615200</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>1.613700</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>1.598500</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>1.573000</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>1.579800</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>1.579500</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>1.571500</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>1.583900</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>1.577200</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>1.569200</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>1.566800</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>1.553500</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>1.584600</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>1.567000</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>1.554600</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>1.553800</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>1.564400</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>1.547300</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>1.574500</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>1.567500</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>1.561400</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>1.566400</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>1.559300</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>1.541300</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>1.556400</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>1.562200</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>1.569600</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>1.551300</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>1.559400</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>1.546500</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>1.552400</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>1.562900</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>1.552100</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>1.541500</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>1.552300</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>1.554500</td>\n","    </tr>\n","    <tr>\n","      <td>68000</td>\n","      <td>1.541600</td>\n","    </tr>\n","    <tr>\n","      <td>68500</td>\n","      <td>1.550200</td>\n","    </tr>\n","    <tr>\n","      <td>69000</td>\n","      <td>1.548800</td>\n","    </tr>\n","    <tr>\n","      <td>69500</td>\n","      <td>1.542700</td>\n","    </tr>\n","    <tr>\n","      <td>70000</td>\n","      <td>1.529700</td>\n","    </tr>\n","    <tr>\n","      <td>70500</td>\n","      <td>1.553900</td>\n","    </tr>\n","    <tr>\n","      <td>71000</td>\n","      <td>1.548300</td>\n","    </tr>\n","    <tr>\n","      <td>71500</td>\n","      <td>1.555200</td>\n","    </tr>\n","    <tr>\n","      <td>72000</td>\n","      <td>1.535900</td>\n","    </tr>\n","    <tr>\n","      <td>72500</td>\n","      <td>1.537900</td>\n","    </tr>\n","    <tr>\n","      <td>73000</td>\n","      <td>1.557000</td>\n","    </tr>\n","    <tr>\n","      <td>73500</td>\n","      <td>1.538900</td>\n","    </tr>\n","    <tr>\n","      <td>74000</td>\n","      <td>1.531300</td>\n","    </tr>\n","    <tr>\n","      <td>74500</td>\n","      <td>1.552500</td>\n","    </tr>\n","    <tr>\n","      <td>75000</td>\n","      <td>1.540300</td>\n","    </tr>\n","    <tr>\n","      <td>75500</td>\n","      <td>1.541000</td>\n","    </tr>\n","    <tr>\n","      <td>76000</td>\n","      <td>1.551800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e846bfc472746e08f0d6cad1b04cf6d"}},"metadata":{}}]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1e846bfc472746e08f0d6cad1b04cf6d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87b2411d0c684cd6a83e3f4ecfa04555","IPY_MODEL_753d14a31bed472ebef84c5b07314c3e","IPY_MODEL_5614c8d6dab54fd6aa3d70350fb639c7"],"layout":"IPY_MODEL_6d727798ccd748eaa8a5ba87c88570b6"}},"87b2411d0c684cd6a83e3f4ecfa04555":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d48991b6c6d40ef85ab13b583de9200","placeholder":"​","style":"IPY_MODEL_6aa877f961ed46dc9906a64d328b102d","value":"Computing widget examples:   0%"}},"753d14a31bed472ebef84c5b07314c3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fca820ac3da4ebfbf4966efd3fe5147","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_492c57272f3246ca8e46833d82238215","value":1}},"5614c8d6dab54fd6aa3d70350fb639c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce126d1b27504318b061e2371ab731fd","placeholder":"​","style":"IPY_MODEL_0f69d181f73e458fae2113616d00ff9e","value":" 0/1 [00:00&lt;?, ?example/s]"}},"6d727798ccd748eaa8a5ba87c88570b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"8d48991b6c6d40ef85ab13b583de9200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aa877f961ed46dc9906a64d328b102d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fca820ac3da4ebfbf4966efd3fe5147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492c57272f3246ca8e46833d82238215":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce126d1b27504318b061e2371ab731fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f69d181f73e458fae2113616d00ff9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}