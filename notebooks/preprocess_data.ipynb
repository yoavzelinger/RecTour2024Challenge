{"cells":[{"cell_type":"markdown","metadata":{"id":"VFuMzfXwCLnZ"},"source":["STEP 0: Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":16452,"status":"ok","timestamp":1734609287498,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"027FCZv7CLna","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b2db9fb-168b-474e-bd16-4bcaa0cf9e6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Adjusting current working directory to parent directory\n","from pathlib import Path\n","from os import chdir\n","from platform import system\n","\n","try:\n","    current_directory\n","except: # First  run - initialize current_directory\n","    current_directory = Path.cwd()\n","    if system() == \"Linux\": # Colab\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        current_directory = f\"{current_directory}/drive/MyDrive/Colab Notebooks/RecTour2024Challenge\"\n","    else:\n","        current_directory = current_directory.parent\n","finally:\n","    chdir(current_directory)\n","\n","\n","\n","# External imports\n","import pandas as pd\n","import numpy as np\n","\n","from random import randint\n","\n","import torch\n","import torch.nn as nn\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","\n","\n","# Internal imports\n","from src.data.csv_tools import csv_to_dataframe, dataframe_to_csv, save_submission\n","from src.data.pickle_tools import save_to_pickle, load_pickle\n","from src.utils.preprocessing_tools import *"]},{"cell_type":"markdown","metadata":{"id":"mgu4_fVFCLnb"},"source":["STEP 1: Merge and aggregate the raw data:\n","    for each accommodation get a list of all the relevant reviews to it\n","    create a single dataset for each set of the train and val - first merge users to match by userid (they are unique), then merge to reviews by review id"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1155,"status":"ok","timestamp":1734609288651,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"FshX5SvJCLnb"},"outputs":[],"source":["# aggregate reviews for each accommodation as a list\n","for set_name in [\"train\", \"val\", \"test\"]:\n","    try:\n","        accommodation_reviews = load_pickle(f\"{set_name}_reviews_grouped_by_accommodation\")\n","    except:\n","        accommodation_reviews = create_accommodation_reviews(set_name)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27953,"status":"ok","timestamp":1734609316603,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"ouOG1Qa9CLnb","outputId":"93c9a412-ad44-4380-f495-4f0f200ad711"},"outputs":[{"output_type":"stream","name":"stdout","text":["train size, val size\n","1628989 1628989\n"]}],"source":["# Concatenate train and val sets\n","train_processed_set, val_processed_set = None, None\n","try:\n","    train_processed_set = csv_to_dataframe(\"train\")\n","except:\n","    train_processed_set = create_concatenated_set(\"train\")\n","try:\n","    val_processed_set = csv_to_dataframe(\"val\")\n","except:\n","    val_processed_set = create_concatenated_set(\"val\")\n","\n","print(\"train size, val size\")\n","print(len(train_processed_set), len(val_processed_set))\n","\n","train_user_review_dict = {}\n","try:\n","    train_user_review_dict = load_pickle(\"train_user_review_dict\")\n","except:\n","    for index, row in train_processed_set.iterrows():\n","        user_id = row[\"user_id\"]\n","        review_id = row[\"review_id\"]\n","        train_user_review_dict[user_id] = review_id\n","    save_to_pickle(train_user_review_dict, \"train_user_review_dict\")\n","\n","val_user_review_dict = {}\n","try:\n","    val_user_review_dict = load_pickle(\"val_user_review_dict\")\n","except:\n","    for index, row in val_processed_set.iterrows():\n","        user_id = row[\"user_id\"]\n","        review_id = row[\"review_id\"]\n","        val_user_review_dict[user_id] = review_id\n","    save_to_pickle(val_user_review_dict, \"val_user_review_dict\")"]},{"cell_type":"markdown","metadata":{"id":"Qxo4m9TxCLnc"},"source":["STEP 2: Create vector for each text of the review"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4378,"status":"ok","timestamp":1734609320979,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"dE-5KkCVCLnc","outputId":"8fde07cc-acb5-4ec4-fa37-bd6438ee210d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values in train:\n","review_title       0\n","review_positive    0\n","review_negative    0\n","dtype: int64\n","Missing values in val:\n","review_title       0\n","review_positive    0\n","review_negative    0\n","dtype: int64\n","Missing values in test:\n","review_title       0\n","review_positive    0\n","review_negative    0\n","dtype: int64\n"]}],"source":["# Fill na with empty strings\n","train_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]] = train_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].fillna(\"\")\n","val_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]] = val_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].fillna(\"\")\n","# For test, get reviews data\n","test_reviews = csv_to_dataframe(\"test\", \"reviews\")\n","test_reviews[[\"review_title\", \"review_positive\", \"review_negative\"]] = test_reviews[[\"review_title\", \"review_positive\", \"review_negative\"]].fillna(\"\")\n","# Validate that there are no missing values\n","print(\"Missing values in train:\")\n","print(train_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].isna().sum())\n","print(\"Missing values in val:\")\n","print(val_processed_set[[\"review_title\", \"review_positive\", \"review_negative\"]].isna().sum())\n","print(\"Missing values in test:\")\n","print(test_reviews[[\"review_title\", \"review_positive\", \"review_negative\"]].isna().sum())"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2651,"status":"ok","timestamp":1734609323627,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"3Jk5vrwLCLnc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b37e9053-faf4-4fcd-f368-f53d2f3a5d95"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["# Initialize the model\n","model = SentenceTransformer(\"all-MiniLM-L6-v2\")"]},{"cell_type":"markdown","metadata":{"id":"9zv0r5TpCLnc"},"source":["If you want to check if it's working on a small dataset run the below cell, otherwise you can skip it"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1086,"status":"ok","timestamp":1734609324711,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"lLM9TGf0CLnc","outputId":"2924fd29-025c-43e5-cfb6-f6f16edda6ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["(5, 384)\n","(5, 384)\n","(5, 384)\n"]}],"source":["# Sanity check\n","sanity_dataset = val_processed_set.head()\n","title_vectors = model.encode(sanity_dataset[\"review_title\"].values)\n","positive_vectors = model.encode(sanity_dataset[\"review_positive\"].values)\n","negative_vectors = model.encode(sanity_dataset[\"review_negative\"].values)\n","print(title_vectors.shape)\n","print(positive_vectors.shape)\n","print(negative_vectors.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734609324711,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"n1UOXxiHCLnc"},"outputs":[],"source":["# Get pairs for each set\n","set_name_data_pair = [(\"train\", train_processed_set), (\"val\", val_processed_set), (\"test\", test_reviews)]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49184,"status":"ok","timestamp":1734609373893,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"EeiBcOhvCLnd","outputId":"482fa003-c565-4b13-c298-4c03abe2699d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting train\n","Finished title.\n","Finished positive.\n","Finished negative.\n","Starting val\n","Finished title.\n","Finished positive.\n","Finished negative.\n","Starting test\n","Finished title.\n","Finished positive.\n","Finished negative.\n"]}],"source":["for set_name, set_data in set_name_data_pair:\n","    print(\"Starting\", set_name)\n","\n","    review_ids = set_data[\"review_id\"].values\n","    try:\n","      load_pickle(f\"{set_name}_title_vectors_dict\")\n","    except:\n","      title_vectors = model.encode(set_data[\"review_title\"].values, show_progress_bar=True)\n","      title_vectors_dict = dict(zip(review_ids, title_vectors))\n","      save_to_pickle(title_vectors_dict, f\"{set_name}_title_vectors_dict\")\n","    print(f\"Finished title.\")\n","\n","    try:\n","      load_pickle(f\"{set_name}_positive_vectors_dict\")\n","    except:\n","      positive_vectors = model.encode(set_data[\"review_positive\"].values, show_progress_bar=True)\n","      positive_review_vectors_dict = dict(zip(review_ids, positive_vectors))\n","      save_to_pickle(positive_review_vectors_dict, f\"{set_name}_positive_vectors_dict\")\n","    print(f\"Finished positive.\")\n","\n","    try:\n","      load_pickle(f\"{set_name}_negative_vectors_dict\")\n","    except:\n","      negative_vectors = model.encode(set_data[\"review_negative\"].values, show_progress_bar=True)\n","      negative_review_vectors_dict = dict(zip(review_ids, negative_vectors))\n","      save_to_pickle(negative_review_vectors_dict, f\"{set_name}_negative_vectors_dict\")\n","    print(f\"Finished negative.\")"]},{"cell_type":"markdown","metadata":{"id":"g1qJMRj9CLnd"},"source":["STEP 3 - Create embeddings for countries"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734609373893,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"5G9g5ULKCLnd"},"outputs":[],"source":["# Get all unique countries from train and val\n","countries_embedding_dict = {}\n","try:\n","    countries_embedding_dict = load_pickle(\"countries_embedding_dict\")\n","except:\n","    train_countries = train_processed_set[\"guest_country\"].unique()\n","    val_countries = val_processed_set[\"guest_country\"].unique()\n","    all_countries = np.unique(np.concatenate((train_countries, val_countries)))\n","    embedding = nn.Embedding(len(all_countries), 8)\n","    for country_index, country in enumerate(all_countries):\n","        country_embedding = embedding(torch.tensor(country_index)).detach().numpy()\n","        countries_embedding_dict[country] = country_embedding\n","    save_to_pickle(countries_embedding_dict, \"countries_embedding_dict\")"]},{"cell_type":"markdown","metadata":{"id":"zRIodJXFCLnd"},"source":["STEP 4 - Encode the guest types"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734609373893,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"_NGdtqSqCLne"},"outputs":[],"source":["# Get unique guest_types\n","guest_types_embedding_dict = {}\n","try:\n","    guest_types_embedding_dict = load_pickle(\"guest_types_embedding_dict\")\n","except:\n","    train_guest_types = train_processed_set[\"guest_type\"].unique()\n","    val_guest_types = val_processed_set[\"guest_type\"].unique()\n","    all_guest_types = np.unique(np.concatenate((train_guest_types, val_guest_types)))\n","    embedding = nn.Embedding(len(all_guest_types), 2)\n","    for guest_type_index, guest_type in enumerate(all_guest_types):\n","        guest_type_embedding = embedding(torch.tensor(guest_type_index)).detach().numpy()\n","        guest_types_embedding_dict[guest_type] = guest_type_embedding"]},{"cell_type":"markdown","metadata":{"id":"W7ZKOpXnCLne"},"source":["STEP 5 - Create user embedding for train and validation"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3709,"status":"ok","timestamp":1734609377600,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"gCWY2wm1CLne"},"outputs":[],"source":["for set_name, set_df in set_name_data_pair[: 2]:\n","  try:\n","      current_users_embeddings_dict = load_pickle(f\"{set_name}_user_embeddings_dict\")\n","  except:\n","    current_users_embeddings_dict = {}\n","    for _, row in set_df.iterrows():\n","        user_id = row[\"user_id\"]\n","        embedded_guest_country = countries_embedding_dict[row[\"guest_country\"]]\n","        embedded_guest_type = guest_types_embedding_dict[row[\"guest_type\"]]\n","        embedded_month = row[\"month\"] / 12\n","        embedded_room_nights = row[\"room_nights\"] / 112\n","        current_users_embeddings_dict[user_id] = np.concatenate((embedded_guest_country, embedded_guest_type, np.array([embedded_month, embedded_room_nights])))\n","    save_to_pickle(current_users_embeddings_dict, f\"{set_name}_user_embeddings_dict\")"]},{"cell_type":"markdown","metadata":{"id":"eyPSS_2gMqAq"},"source":["STEP 6 - Training and validation preprocessing"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4880,"status":"ok","timestamp":1734609382479,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"wrCNSDIOCLnf","outputId":"d86df3c1-a9e7-4fb2-e8e4-efe797c55a58"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1628989, 12)\n","(203787, 12)\n"]}],"source":["# Create np array that each cell contains user embedding\n","train_user_embeddings_array = np.array(list(load_pickle(\"train_user_embeddings_dict\").values()))\n","print(train_user_embeddings_array.shape)\n","val_user_embeddings_array = np.array(list(load_pickle(\"val_user_embeddings_dict\").values()))\n","print(val_user_embeddings_array.shape)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":21657,"status":"ok","timestamp":1734609404134,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"9x_R1xsVCLnf"},"outputs":[],"source":["# Load all section reviews embeddigns\n","train_title_vectors_dict = load_pickle(\"train_title_vectors_dict\")\n","train_positive_vectors_dict = load_pickle(\"train_positive_vectors_dict\")\n","train_negative_vectors_dict = load_pickle(\"train_negative_vectors_dict\")\n","val_title_vectors_dict = load_pickle(\"val_title_vectors_dict\")\n","val_positive_vectors_dict = load_pickle(\"val_positive_vectors_dict\")\n","val_negative_vectors_dict = load_pickle(\"val_negative_vectors_dict\")"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4987,"status":"ok","timestamp":1734609409119,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"MO2d70LSpFkH"},"outputs":[],"source":["reviews_embeddings_arrays = {\n","    \"train\": {\n","        \"users\": train_user_embeddings_array,\n","        \"title\": np.array(list(train_title_vectors_dict.values())),\n","        \"positive\": np.array(list(train_positive_vectors_dict.values())),\n","        \"negative\": np.array(list(train_negative_vectors_dict.values())),\n","        \"labels\": np.concatenate([np.ones(len(train_user_embeddings_array)), np.zeros(len(train_user_embeddings_array) * 2)], axis=0)\n","    },\n","    \"val\": {\n","        \"users\": val_user_embeddings_array,\n","        \"title\": np.array(list(val_title_vectors_dict.values())),\n","        \"positive\": np.array(list(val_positive_vectors_dict.values())),\n","        \"negative\": np.array(list(val_negative_vectors_dict.values())),\n","        \"labels\": np.concatenate([np.ones(len(val_user_embeddings_array)), np.zeros(len(val_user_embeddings_array) * 2)], axis=0)\n","    }\n","}"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1734609409119,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"YCG2xxZpkEOG"},"outputs":[],"source":["# Helper functions\n","def get_correct_pairs(set_name, section_name):\n","    return reviews_embeddings_arrays[set_name][\"users\"], reviews_embeddings_arrays[set_name][section_name]\n","\n","def get_set_labels(section_name):\n","    return reviews_embeddings_arrays[section_name][\"labels\"]\n","\n","def create_double_negative_lists(embeddings_list, set_name=\"\", section_name=\"\"):\n","    amount_of_reviews = len(embeddings_list)\n","    last_review_index = amount_of_reviews - 1\n","    print_steps = amount_of_reviews // 20\n","    current_print = 0\n","    negative_reviews1, negative_reviews2 = [], []\n","    for current_review_index, current_embedding in enumerate(embeddings_list, start=1):\n","        if current_review_index % print_steps == 0:\n","            print(f\"{set_name}-{section_name}: {current_print}% completed\")\n","            current_print += 5\n","        negative_review1 = embeddings_list[randint(0, last_review_index)]\n","        while np.all(negative_review1 == current_embedding):\n","            # print(\"repeated1\")\n","            negative_review1 = embeddings_list[randint(0, last_review_index)]\n","        negative_review2 = embeddings_list[randint(0, last_review_index)]\n","        while np.all(negative_review2 == current_embedding) or np.all(negative_review1 == negative_review2):\n","            # print(\"repeated2\")\n","            negative_review2 = embeddings_list[randint(0, last_review_index)]\n","        negative_reviews1.append(negative_review1)\n","        negative_reviews2.append(negative_review2)\n","    return np.array(negative_reviews1), np.array(negative_reviews2)"]},{"cell_type":"markdown","metadata":{"id":"eQzLcdUTsBjN"},"source":["STEP 7 - Creating the model"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734609409119,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"},"user_tz":-120},"id":"6F6X1jRlWl3y"},"outputs":[],"source":["def contrastive_loss(y_true, y_pred, margin=1.0):\n","    square_pred = tf.square(y_pred)\n","    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n","    return tf.reduce_mean((1 - y_true) * square_pred + y_true * margin_square)\n","\n","def create_contrastive_model():\n","    user_input = Input(shape=(12,), name='user')\n","    user_dense = Dense(384, activation='relu', name='user_dense')(user_input)\n","\n","    review_input = Input(shape=(384,), name='review')\n","\n","    cosine_similarity = Lambda(lambda tensors: tf.reduce_sum(tensors[0] * tensors[1], axis=-1, keepdims=True)) \\\n","                              ([user_dense, review_input])\n","\n","    return Model([user_input, review_input], cosine_similarity, name='contrastive_model')\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n","\n","def create_section_model(section_name, batch_size=512, epochs=15):\n","    print(\"Creating training data\")\n","    train_users, train_section_reviews = get_correct_pairs(\"train\", section_name)\n","    train_negative1, train_negative2 = create_double_negative_lists(train_section_reviews, \"train\", section_name)\n","    train_all_users = np.concatenate([train_users, train_users, train_users], axis=0)\n","    train_all_section_reviews = np.concatenate([train_section_reviews, train_negative1, train_negative2], axis=0)\n","    train_all_labels = get_set_labels(\"train\")\n","    print(\"Creating validation data\")\n","    val_users, val_section_reviews = get_correct_pairs(\"val\", section_name)\n","    val_negative1, val_negative2 = create_double_negative_lists(val_section_reviews, \"val\", section_name)\n","    val_all_users = np.concatenate([val_users, val_users, val_users], axis=0)\n","    val_all_section_reviews = np.concatenate([val_section_reviews, val_negative1, val_negative2], axis=0)\n","    val_all_labels = get_set_labels(\"val\")\n","\n","\n","    model = create_contrastive_model()\n","\n","    model.compile(optimizer='adam', loss=contrastive_loss)\n","\n","    history = model.fit(\n","        [train_all_users, train_all_section_reviews], train_all_labels,\n","        batch_size=batch_size,\n","        epochs=epochs,\n","        validation_data=([val_all_users, val_all_section_reviews], val_all_labels),\n","        callbacks=[early_stopping, lr_scheduler]\n","    )\n","    return model"]},{"cell_type":"code","source":["for section_name in [\"title\", \"positive\", \"negative\"]:\n","    section_model = create_section_model(section_name)\n","    section_model.save(f\"{section_name}_model.h5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cijhh-QouQgt","executionInfo":{"status":"ok","timestamp":1734610124124,"user_tz":-120,"elapsed":715007,"user":{"displayName":"Yoav Zelinger","userId":"13221241970274122758"}},"outputId":"5705a106-2e48-4a27-f776-fc75ba440d7d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating training data\n","train-title: 0% completed\n","train-title: 5% completed\n","train-title: 10% completed\n","train-title: 15% completed\n","train-title: 20% completed\n","train-title: 25% completed\n","train-title: 30% completed\n","train-title: 35% completed\n","train-title: 40% completed\n","train-title: 45% completed\n","train-title: 50% completed\n","train-title: 55% completed\n","train-title: 60% completed\n","train-title: 65% completed\n","train-title: 70% completed\n","train-title: 75% completed\n","train-title: 80% completed\n","train-title: 85% completed\n","train-title: 90% completed\n","train-title: 95% completed\n","Creating validation data\n","val-title: 0% completed\n","val-title: 5% completed\n","val-title: 10% completed\n","val-title: 15% completed\n","val-title: 20% completed\n","val-title: 25% completed\n","val-title: 30% completed\n","val-title: 35% completed\n","val-title: 40% completed\n","val-title: 45% completed\n","val-title: 50% completed\n","val-title: 55% completed\n","val-title: 60% completed\n","val-title: 65% completed\n","val-title: 70% completed\n","val-title: 75% completed\n","val-title: 80% completed\n","val-title: 85% completed\n","val-title: 90% completed\n","val-title: 95% completed\n","Epoch 1/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 0.2141 - val_loss: 0.2097 - learning_rate: 0.0010\n","Epoch 2/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2093 - val_loss: 0.2092 - learning_rate: 0.0010\n","Epoch 3/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2086 - val_loss: 0.2089 - learning_rate: 0.0010\n","Epoch 4/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2085 - val_loss: 0.2086 - learning_rate: 0.0010\n","Epoch 5/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2083 - val_loss: 0.2087 - learning_rate: 0.0010\n","Epoch 6/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2081 - val_loss: 0.2085 - learning_rate: 0.0010\n","Epoch 7/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2081 - val_loss: 0.2084 - learning_rate: 0.0010\n","Epoch 8/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2080 - val_loss: 0.2082 - learning_rate: 0.0010\n","Epoch 9/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2079 - val_loss: 0.2088 - learning_rate: 0.0010\n","Epoch 10/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2079 - val_loss: 0.2082 - learning_rate: 0.0010\n","Epoch 11/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2078 - val_loss: 0.2084 - learning_rate: 0.0010\n","Epoch 12/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2073 - val_loss: 0.2077 - learning_rate: 5.0000e-04\n","Epoch 13/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2071 - val_loss: 0.2076 - learning_rate: 5.0000e-04\n","Epoch 14/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2073 - val_loss: 0.2077 - learning_rate: 5.0000e-04\n","Epoch 15/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2072 - val_loss: 0.2075 - learning_rate: 5.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Creating training data\n","train-positive: 0% completed\n","train-positive: 5% completed\n","train-positive: 10% completed\n","train-positive: 15% completed\n","train-positive: 20% completed\n","train-positive: 25% completed\n","train-positive: 30% completed\n","train-positive: 35% completed\n","train-positive: 40% completed\n","train-positive: 45% completed\n","train-positive: 50% completed\n","train-positive: 55% completed\n","train-positive: 60% completed\n","train-positive: 65% completed\n","train-positive: 70% completed\n","train-positive: 75% completed\n","train-positive: 80% completed\n","train-positive: 85% completed\n","train-positive: 90% completed\n","train-positive: 95% completed\n","Creating validation data\n","val-positive: 0% completed\n","val-positive: 5% completed\n","val-positive: 10% completed\n","val-positive: 15% completed\n","val-positive: 20% completed\n","val-positive: 25% completed\n","val-positive: 30% completed\n","val-positive: 35% completed\n","val-positive: 40% completed\n","val-positive: 45% completed\n","val-positive: 50% completed\n","val-positive: 55% completed\n","val-positive: 60% completed\n","val-positive: 65% completed\n","val-positive: 70% completed\n","val-positive: 75% completed\n","val-positive: 80% completed\n","val-positive: 85% completed\n","val-positive: 90% completed\n","val-positive: 95% completed\n","Epoch 1/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 0.2141 - val_loss: 0.2071 - learning_rate: 0.0010\n","Epoch 2/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2056 - val_loss: 0.2059 - learning_rate: 0.0010\n","Epoch 3/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2047 - val_loss: 0.2052 - learning_rate: 0.0010\n","Epoch 4/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2042 - val_loss: 0.2044 - learning_rate: 0.0010\n","Epoch 5/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2040 - val_loss: 0.2048 - learning_rate: 0.0010\n","Epoch 6/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2036 - val_loss: 0.2042 - learning_rate: 0.0010\n","Epoch 7/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2033 - val_loss: 0.2039 - learning_rate: 0.0010\n","Epoch 8/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2034 - val_loss: 0.2040 - learning_rate: 0.0010\n","Epoch 9/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2030 - val_loss: 0.2036 - learning_rate: 0.0010\n","Epoch 10/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2030 - val_loss: 0.2040 - learning_rate: 0.0010\n","Epoch 11/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2028 - val_loss: 0.2040 - learning_rate: 0.0010\n","Epoch 12/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2030 - val_loss: 0.2035 - learning_rate: 0.0010\n","Epoch 13/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2021 - val_loss: 0.2031 - learning_rate: 5.0000e-04\n","Epoch 14/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2020 - val_loss: 0.2028 - learning_rate: 5.0000e-04\n","Epoch 15/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2020 - val_loss: 0.2028 - learning_rate: 5.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Creating training data\n","train-negative: 0% completed\n","train-negative: 5% completed\n","train-negative: 10% completed\n","train-negative: 15% completed\n","train-negative: 20% completed\n","train-negative: 25% completed\n","train-negative: 30% completed\n","train-negative: 35% completed\n","train-negative: 40% completed\n","train-negative: 45% completed\n","train-negative: 50% completed\n","train-negative: 55% completed\n","train-negative: 60% completed\n","train-negative: 65% completed\n","train-negative: 70% completed\n","train-negative: 75% completed\n","train-negative: 80% completed\n","train-negative: 85% completed\n","train-negative: 90% completed\n","train-negative: 95% completed\n","Creating validation data\n","val-negative: 0% completed\n","val-negative: 5% completed\n","val-negative: 10% completed\n","val-negative: 15% completed\n","val-negative: 20% completed\n","val-negative: 25% completed\n","val-negative: 30% completed\n","val-negative: 35% completed\n","val-negative: 40% completed\n","val-negative: 45% completed\n","val-negative: 50% completed\n","val-negative: 55% completed\n","val-negative: 60% completed\n","val-negative: 65% completed\n","val-negative: 70% completed\n","val-negative: 75% completed\n","val-negative: 80% completed\n","val-negative: 85% completed\n","val-negative: 90% completed\n","val-negative: 95% completed\n","Epoch 1/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - loss: 0.2189 - val_loss: 0.2136 - learning_rate: 0.0010\n","Epoch 2/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2130 - val_loss: 0.2128 - learning_rate: 0.0010\n","Epoch 3/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2124 - val_loss: 0.2127 - learning_rate: 0.0010\n","Epoch 4/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2122 - val_loss: 0.2129 - learning_rate: 0.0010\n","Epoch 5/15\n","\u001b[1m9545/9545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.2121 - val_loss: 0.2122 - learning_rate: 0.0010\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"school_venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}